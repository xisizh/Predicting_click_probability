import mathclass EvalUtil:    # =====================  # Evaluates the model by computing the weighted rooted mean square error of  # between the prediction and the true labels.  #   # @param path_to_sol  # @param ctr_predictions  # @return the weighted rooted mean square error of between the prediction  #         and the true labels.  # ======================  @classmethod  def eval(cls, path_to_sol, ctr_predictions):    size = len(ctr_predictions)    wmse = 0.0    with open(path_to_sol, 'r') as f:      for i, line in enumerate(f):        ctr = float(line)        wmse += pow((ctr - ctr_predictions[i]), 2)    return math.sqrt(wmse / size)  # ========================  # Evaluate the model by calculating the precision recall, and F-score based on the previous two.  #  # @param path_to_sol  # @param ctr_predictions  # @return the weighted rooted mean square error of between the prediction  #         and the true labels.  # ======================  @classmethod  def error_analysis(cls, path_to_sol, ctr_predictions):    perform = {"true_pos":0,"true_neg":0,"false_pos":0,"false_neg":0}    with open(path_to_sol, 'r') as f:      for i, line in enumerate(f):        ctr = float(line)        #ctr = 1 if ctr > 0.5 else 0        #ctr_predictions[i] = 1 if ctr_predictions[i] > 0.5 else 0        if ctr > 0.5 and ctr_predictions[i] > 0.5: perform['true_pos'] += 1        elif ctr > 0.5 and ctr_predictions[i] < 0.5: perform['false_neg'] += 1        elif ctr < 0.5 and ctr_predictions[i] > 0.5: perform['false_pos'] += 1        else: perform['true_neg'] += 1    print("The performance :", perform)    precision = 0 if perform['true_pos']==0 else perform['true_pos']/(perform['true_pos']+perform['false_pos'])    recall = 0 if perform['true_pos']==0 else perform['true_pos']/(perform['true_pos'] + perform['false_neg'])    print("Precision is :", precision)    print("Recall (True positive rate) is :", recall)    print("False positive rate is :", perform['false_pos']/(perform['false_pos']+perform['true_neg']))    F_score = 0 if perform['true_pos']==0 else 2.0*precision*recall/(precision+recall)    return F_score    # =====================  # Evaluates the model by computing the weighted rooted mean square error of  # between the prediction and the true labels.  #   # @param path_to_sol  # @param path_to_predictions  # @return the weighted rooted mean square error of between the prediction  #         and the true labels.  # =====================  @classmethod  def eval_paths(cls, path_to_sol, path_to_predictions):    ctr_predictions = []    with open(path_to_predictions, 'r') as f:      for line in f:        ctr_predictions.append(float(line))    return cls.eval(path_to_sol, ctr_predictions)  # =====================  # Evaluates the model by computing the weighted rooted mean square error of  # between the prediction and the true labels.  # @param path_to_sol {String} path to solution  # @param average_ctr {Float} average CTR  # @return {Float} baseline RMSE  # =====================  @classmethod  def eval_baseline(cls, path_to_sol, average_ctr):    rmse = 0.0    count = 0    with open(path_to_sol, 'r') as f:      for line in f:        ctr = float(line)        rmse += math.pow(ctr - average_ctr, 2)        count += 1    return math.sqrt(rmse / count)    # =====================  # Evaluates the model by computing the weighted rooted mean square error of  # between the prediction and the true labels, using the including list to decide whether the each  # test data point should be included in evaluation.  #   # This is useful for evaluating the RMSE on a subset of test data (with common users ...).  # @param path_to_sol  # @param ctr_predictions  # @param including_list  # @return  # =====================  @classmethod  def eval_with_including_list(cls, path_to_sol, ctr_predictions, including_list):    wmse = 0.0    total = 0    perform = {"true_pos": 0, "true_neg": 0, "false_pos": 0, "false_neg": 0}    with open(path_to_sol, 'r') as f:      for i, line in enumerate(f):        if not including_list[i]:          continue        ctr = float(line)        wmse += pow((ctr - ctr_predictions[i]), 2)        total += 1        if ctr > 0.5 and ctr_predictions[i] > 0.5: perform['true_pos'] += 1        elif ctr > 0.5 and ctr_predictions[i] < 0.5: perform['false_neg'] += 1        elif ctr < 0.5 and ctr_predictions[i] > 0.5: perform['false_pos'] += 1        else: perform['true_neg'] += 1    print("The performance :", perform)    precision = 0.0 if perform['true_pos'] == 0 else perform['true_pos'] / (perform['true_pos'] + perform['false_pos'])    recall = 0.0 if perform['true_pos'] == 0 else perform['true_pos'] / (perform['true_pos'] + perform['false_neg'])    print("Precision is :", precision)    print("Recall (True positive rate) is :", recall)    FPR = 0.0 if perform['false_pos']==0 else perform['false_pos'] / (perform['false_pos'] + perform['true_neg'])    print("False positive rate is :", FPR)    F_score = 0.0 if perform['true_pos'] == 0 else 2.0 * precision * recall / (precision + recall)    print("F-score for the common users is :", F_score)    print("total count is :", total,'\n')    return math.sqrt(wmse / total)